<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Activation Functions in AI | doretox</title>
<meta name=keywords content="AI"><meta name=description content="Understanding Activation Functions in AI Types and Examples with Python Code"><meta name=author content="doreox"><link rel=canonical href=https://doretox.github.io/posts/activation-function/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://doretox.github.io/%5Ccontent%5Cimages%5Cfavicon-16.png><link rel=icon type=image/png sizes=16x16 href=https://doretox.github.io/%5Ccontent%5Cimages%5Cfavicon-16.png><link rel=icon type=image/png sizes=32x32 href=https://doretox.github.io/%5Ccontent%5Cimages%5Cfavicon-32.png><link rel=apple-touch-icon href=https://doretox.github.io/%5Ccontent%5Cimages%5Cfavicon-16.png><link rel=mask-icon href=https://doretox.github.io/%5Ccontent%5Cimages%5Cfavicon-16.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://doretox.github.io/posts/activation-function/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-166484777-1","auto"),ga("send","pageview"))</script><meta property="og:title" content="Activation Functions in AI"><meta property="og:description" content="Understanding Activation Functions in AI Types and Examples with Python Code"><meta property="og:type" content="article"><meta property="og:url" content="https://doretox.github.io/posts/activation-function/"><meta property="og:image" content="https://doretox.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-04-25T00:00:00+05:30"><meta property="article:modified_time" content="2023-04-25T00:00:00+05:30"><meta property="og:site_name" content="doretox"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://doretox.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Activation Functions in AI"><meta name=twitter:description content="Understanding Activation Functions in AI Types and Examples with Python Code"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://doretox.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Activation Functions in AI","item":"https://doretox.github.io/posts/activation-function/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Activation Functions in AI","name":"Activation Functions in AI","description":"Understanding Activation Functions in AI Types and Examples with Python Code","keywords":["AI"],"articleBody":"In neural networks and artificial intelligence, activation functions are critical. They are mathematical functions applied to each neuron’s output in a neural network to transform it into the desired output range.\nTypes of Activation Functions There are several types of activation functions, but we will focus on the most commonly used ones.\nSigmoid Function The sigmoid function is a popular activation function that produces an output in the 0 to 1 range. It is especially beneficial for binary classification problems. The sigmoid function is defined as:\nwhere x is the input to the function.\nReLU Function The ReLU (Rectified Linear Unit) function is another commonly used activation function. It is particularly useful for deep neural networks. The ReLU function is defined as:\nReLU(x)=max(0,x) where x is the input to the function.\nSoftmax Function The Softmax function is used in multi-class classification problems, where the output can belong to one of several classes. It normalizes the output so that it adds up to 1. The Softmax function is defined as:\nwhere z is the input to the function.\nExamples of Activation Functions Image Classification The input to the neural network in image classification is an image, and the output is a label that describes the image. The ReLU activation function is frequently used in the network’s hidden layers in this application, while the Softmax activation function is used in the output layer to generate the probability distribution over the possible labels.\nSentiment Analysis The input in sentiment analysis is a piece of text, and the output is a classification of the text’s sentiment (positive, negative, or neutral). The sigmoid activation function is frequently used in the output layer of this application to generate a probability distribution over the possible sentiment labels.\nPython Example of an Activation Function Here is an example of how the ReLU activation function works in Python:\ndef relu(x): return max(0, x) # Test the ReLU function with some sample inputs print(relu(2)) # Output: 2 print(relu(-2)) # Output: 0 In this example, the relu() function takes an input x and returns the ReLU output, which is the maximum of 0 and x. When x is positive, the output is the same as x, while when x is negative, the output is 0.\nConclusion Activation functions are an essential component of neural networks and play a critical role in AI model accuracy. In this blog post, we discussed the various types of activation functions and provided examples of how they can be used in AI applications. AI practitioners can improve their models and achieve better performance by understanding activation functions.\nReferences Goodfellow, I., Bengio, Y., \u0026 Courville, A. (2016). Deep Learning. MIT Press. Aggarwal, C. C. (2018). Neural Networks and Deep Learning: A Textbook. Springer. Nielsen, M. (2015). Neural Networks and Deep Learning. Determination Press. PyTorch documentation: Activation Functions. (2021). Retrieved from https://pytorch.org/docs/stable/nn.functional.html#non-linear-activations-weighted-sum-nonlinearity. ","wordCount":"474","inLanguage":"en","datePublished":"2023-04-25T00:00:00+05:30","dateModified":"2023-04-25T00:00:00+05:30","author":{"@type":"Person","name":"doreox"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://doretox.github.io/posts/activation-function/"},"publisher":{"@type":"Organization","name":"doretox","logo":{"@type":"ImageObject","url":"https://doretox.github.io/%5Ccontent%5Cimages%5Cfavicon-16.png"}}}</script></head><body class=dark id=top><header class=header><nav class=nav><div class=logo><a href=https://doretox.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://doretox.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://doretox.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://doretox.github.io/about/ title=About><span>About</span></a></li><li><a href=https://doretox.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://doretox.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://doretox.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Activation Functions in AI</h1><div class=post-description>Understanding Activation Functions in AI Types and Examples with Python Code</div><div class=post-meta><span title='2023-04-25 00:00:00 +0530 +0530'>April 25, 2023</span>&nbsp;·&nbsp;doreox</div></header><div class=post-content><p>In neural networks and artificial intelligence, activation functions are critical. They are mathematical functions applied to each neuron&rsquo;s output in a neural network to transform it into the desired output range.</p><h2 id=types-of-activation-functions>Types of Activation Functions<a hidden class=anchor aria-hidden=true href=#types-of-activation-functions>#</a></h2><p>There are several types of activation functions, but we will focus on the most commonly used ones.</p><h3 id=sigmoid-function>Sigmoid Function<a hidden class=anchor aria-hidden=true href=#sigmoid-function>#</a></h3><p>The sigmoid function is a popular activation function that produces an output in the 0 to 1 range. It is especially beneficial for binary classification problems. The sigmoid function is defined as:</p><p><img loading=lazy src=/images/activation-function/sigmoid_eq.png alt="Sigmoid Equation."></p><p><img loading=lazy src=/images/activation-function/sigmoid_graph.png alt="Sigmoid Graph."></p><p>where x is the input to the function.</p><h3 id=relu-function>ReLU Function<a hidden class=anchor aria-hidden=true href=#relu-function>#</a></h3><p>The ReLU (Rectified Linear Unit) function is another commonly used activation function. It is particularly useful for deep neural networks. The ReLU function is defined as:</p><pre tabindex=0><code class=language-math data-lang=math>ReLU(x)=max(0,x)
</code></pre><p>where x is the input to the function.</p><p><img loading=lazy src=/images/activation-function/relu_graph.png alt="Relu Graph."></p><h3 id=softmax-function>Softmax Function<a hidden class=anchor aria-hidden=true href=#softmax-function>#</a></h3><p>The Softmax function is used in multi-class classification problems, where the output can belong to one of several classes. It normalizes the output so that it adds up to 1. The Softmax function is defined as:</p><p><img loading=lazy src=/images/activation-function/softmax_eq.png alt="Softmax Equation.">
where z is the input to the function.</p><p><img loading=lazy src=/images/activation-function/softmax_graph.jpg alt="Softmax Graph."></p><h2 id=examples-of-activation-functions>Examples of Activation Functions<a hidden class=anchor aria-hidden=true href=#examples-of-activation-functions>#</a></h2><h3 id=image-classification>Image Classification<a hidden class=anchor aria-hidden=true href=#image-classification>#</a></h3><p>The input to the neural network in image classification is an image, and the output is a label that describes the image. The ReLU activation function is frequently used in the network&rsquo;s hidden layers in this application, while the Softmax activation function is used in the output layer to generate the probability distribution over the possible labels.</p><h3 id=sentiment-analysis>Sentiment Analysis<a hidden class=anchor aria-hidden=true href=#sentiment-analysis>#</a></h3><p>The input in sentiment analysis is a piece of text, and the output is a classification of the text&rsquo;s sentiment (positive, negative, or neutral). The sigmoid activation function is frequently used in the output layer of this application to generate a probability distribution over the possible sentiment labels.</p><h4 id=python-example-of-an-activation-function>Python Example of an Activation Function<a hidden class=anchor aria-hidden=true href=#python-example-of-an-activation-function>#</a></h4><p>Here is an example of how the ReLU activation function works in Python:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>relu</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>max</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Test the ReLU function with some sample inputs</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>relu</span><span class=p>(</span><span class=mi>2</span><span class=p>))</span>   <span class=c1># Output: 2</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>relu</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>))</span>  <span class=c1># Output: 0</span>
</span></span></code></pre></div><p>In this example, the <code>relu()</code> function takes an input <code>x</code> and returns the ReLU output, which is the maximum of <code>0</code> and <code>x</code>. When <code>x</code> is positive, the output is the same as <code>x</code>, while when <code>x</code> is negative, the output is <code>0</code>.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Activation functions are an essential component of neural networks and play a critical role in AI model accuracy. In this blog post, we discussed the various types of activation functions and provided examples of how they can be used in AI applications. AI practitioners can improve their models and achieve better performance by understanding activation functions.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li>Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
Aggarwal, C. C. (2018). Neural Networks and Deep Learning: A Textbook. Springer.</li><li>Nielsen, M. (2015). Neural Networks and Deep Learning. Determination Press.</li><li>PyTorch documentation: Activation Functions. (2021). Retrieved from <a href=https://pytorch.org/docs/stable/nn.functional.html#non-linear-activations-weighted-sum-nonlinearity>https://pytorch.org/docs/stable/nn.functional.html#non-linear-activations-weighted-sum-nonlinearity</a>.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://doretox.github.io/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://doretox.github.io/posts/network-services-2-walkthrough/><span class=title>« Prev</span><br><span>TryHackMe: Network Services 2 Walkthrough</span>
</a><a class=next href=https://doretox.github.io/posts/av-and-edr/><span class=title>Next »</span><br><span>Endpoint Detection and Response (EDR) vs Antivirus: Understanding the Differences</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://doretox.github.io/>doretox</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>